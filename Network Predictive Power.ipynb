{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthetic_data as sd\n",
    "import network_builder as nb\n",
    "import networkx as nx\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sd.synthetic_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data.shape[1]\n",
    "m = nb.weighted_matrix(data[:999,:])\n",
    "G, c = nb.graph_builder_limit(m, 0.3)\n",
    "G = G.to_undirected()\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "degree = dict(G.degree())\n",
    "closseness = nx.closeness_centrality(G)\n",
    "kcore = dict(nx.core_number(G))\n",
    "betweeness= dict(nx.betweenness_centrality(G))\n",
    "pagerank = dict(nx.pagerank(G, alpha=0.85))\n",
    "eigenvector = dict(nx.eigenvector_centrality(G, max_iter = 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(n):\n",
    "    if (i in degree.keys()):\n",
    "        x = []\n",
    "        #x.append(i)\n",
    "        x.append(degree[i])\n",
    "        x.append(closseness[i])\n",
    "        x.append(kcore[i])\n",
    "        x.append(betweeness[i])\n",
    "        x.append(pagerank[i])\n",
    "        x.append(eigenvector[i])\n",
    "        x.append(nx.clustering(G, i))\n",
    "        X.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[999,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 25, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(50, 25, 10, ), random_state=1)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06490654604107782"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y,y) #0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.722089468918057e-13"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y,y) #0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09408914142801561"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y,y) #0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y,y) #0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06490654604107782"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y,y) #0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100)\n",
    "regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = regr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28165933123468556"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y,y) #0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "import math\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from scipy import stats\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "f_temp = Dataset('temp0818.nc')\n",
    "f_pre = Dataset('pre0818.nc')\n",
    "\n",
    "air = f_temp.variables['air']\n",
    "pr_wtr = f_pre.variables['pr_wtr']\n",
    "\n",
    "N = f_temp['time'].shape[0]\n",
    "\n",
    "air = np.flip(air, 1)\n",
    "pr_wtr = np.flip(pr_wtr, 1)\n",
    "\n",
    "f_spei = Dataset('spei12.nc')\n",
    "spei_data = f_spei.variables['spei']\n",
    "\n",
    "def weighted_matrix(data, spei_data):\n",
    "    lat_number = data.shape[1]\n",
    "    lon_number = data.shape[2]\n",
    "    N = lat_number * lon_number\n",
    "    pearson_r = np.zeros((N,N))\n",
    "    \n",
    "    for i in range(N):\n",
    "        lat_index = math.floor(i/lon_number)\n",
    "        lon_index = i%lon_number\n",
    "        if ma.is_masked(data[-1,lat_index ,lon_index]) or np.isnan(spei_data[-1,lat_index,lon_index]): \n",
    "            pearson_r[i,:] = np.nan\n",
    "            continue;\n",
    "        for j in range(N):\n",
    "            lat_index_sec = math.floor(j/lon_number)\n",
    "            lon_index_sec = j%lon_number\n",
    "            if ma.is_masked(data[-1,lat_index_sec ,lon_index_sec]) or np.isnan(spei_data[-1,lat_index_sec,lon_index_sec]) or data[-1,lat_index_sec ,lon_index_sec]==-9.96921e+36:\n",
    "                pearson_r[i,j] = np.nan\n",
    "            else:\n",
    "                pearson_r[i,j] = stats.pearsonr(data[:,lat_index ,lon_index],data[:,lat_index_sec ,lon_index_sec])[0]\n",
    "                \n",
    "    new = int(math.sqrt((pearson_r.shape[0] * pearson_r.shape[1]) - np.count_nonzero(np.isnan(pearson_r))))\n",
    "    pearson_r_clean = pearson_r[~np.isnan(pearson_r)].reshape((new,new))\n",
    "    return(pearson_r, pearson_r_clean)\n",
    "\n",
    "def graph_builder (weighted_matrix):\n",
    "    weighted_matrix = np.exp(-np.sqrt(1 - weighted_matrix))\n",
    "    componenets_number = 0\n",
    "    limit = 1.0\n",
    "    while componenets_number != 1:\n",
    "        limit -= 0.01\n",
    "        adjacency_matrix = np.zeros(weighted_matrix.shape)\n",
    "        adjacency_matrix[weighted_matrix >= limit] = 1\n",
    "        G = nx.from_numpy_matrix(adjacency_matrix)\n",
    "        G = G.to_undirected()\n",
    "        Gcc = sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
    "        componenets_number = len(Gcc)\n",
    "    return(G)\n",
    "\n",
    "def degree_distribution(G):\n",
    "    vk = dict(G.degree())\n",
    "    vk = list(vk.values()) # we get only the degree values\n",
    "    maxk = np.max(vk)\n",
    "    mink = np.min(min)\n",
    "    kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "    Pk = np.zeros(maxk+1) # P(k)\n",
    "    for k in vk:\n",
    "        Pk[k] = Pk[k] + 1\n",
    "    Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "    return kvalues,Pk\n",
    "\n",
    "def momment_of_degree_distribution(G,m):\n",
    "    k,Pk = degree_distribution(G)\n",
    "    M = sum((k**m)*Pk)\n",
    "    return M\n",
    "\n",
    "def shannon_entropy(G):\n",
    "    k,Pk = degree_distribution(G)\n",
    "    H = 0\n",
    "    for p in Pk:\n",
    "        if(p > 0):\n",
    "            H = H - p*math.log(p, 2)\n",
    "    return H\n",
    "\n",
    "\n",
    "curr_year = 2008\n",
    "spei_start = 1295\n",
    "\n",
    "year = []\n",
    "average_degree_pr = []\n",
    "second_moment_pr = []\n",
    "variance_pr = []\n",
    "shannon_entropy_pr = []\n",
    "transitivity_pr = []\n",
    "average_cluster_pr = []\n",
    "average_shortest_path_length_pr = []\n",
    "diameter_pr = []\n",
    "global_efficiency_pr = []\n",
    "local_efficiency_pr = []\n",
    "average_closeness_pr = []\n",
    "average_betweennes_pr = []\n",
    "average_eigenvector_pr = []\n",
    "average_pagerank_pr = []\n",
    "assortativity_pr = []\n",
    "\n",
    "average_degree_air = []\n",
    "second_moment_air = []\n",
    "variance_air = []\n",
    "shannon_entropy_air = []\n",
    "transitivity_air = []\n",
    "average_cluster_air = []\n",
    "average_shortest_path_length_air = []\n",
    "diameter_air = []\n",
    "global_efficiency_air = []\n",
    "local_efficiency_air = []\n",
    "average_closeness_air = []\n",
    "average_betweennes_air = []\n",
    "average_eigenvector_air = []\n",
    "average_pagerank_air = []\n",
    "assortativity_air = []\n",
    "\n",
    "lon = np.arange(-40.0,72.0,2.5)\n",
    "lat = np.arange(-50.0,51.0,2.5)\n",
    "X = []\n",
    "lat_number = len(lat)\n",
    "lon_number = len(lon)\n",
    "\n",
    "for z in range(math.floor(N/365)):\n",
    "    \n",
    "    spei = np.zeros((lat_number,lon_number))\n",
    "    for i in range(79,280,5):\n",
    "        for j in range(279,500,5):\n",
    "            lat_index = int((i-79)/5)\n",
    "            lon_index = int((j-279)/5)\n",
    "            spei_index = spei_start + z*12\n",
    "            if ma.is_masked(spei_data[-1,i,j]) or ma.is_masked(spei_data[-1,i,j+1]) or ma.is_masked(spei_data[-1,i+1,j]) or ma.is_masked(spei_data[-1,i+1,j+1]):\n",
    "                spei[lat_index,lon_index]= np.nan\n",
    "            else:\n",
    "                spei[lat_index,lon_index] = (np.mean(spei_data[spei_index:spei_index + 12,i,j]) + np.mean(spei_data[spei_index:spei_index + 12,i,j+1]) \n",
    "                                           + np.mean(spei_data[spei_index:spei_index + 12,i+1,j]) + np.mean(spei_data[spei_index:spei_index + 12,i+1,j+1]))/4\n",
    "    \n",
    "    pr_matrix, pr_weighted_matrix = weighted_matrix(pr_wtr[z*365:z*365 + 365,:,:], spei_data)\n",
    "       \n",
    "    G_pr = graph_builder(pr_weighted_matrix)\n",
    "    G_pr = G_pr.to_undirected()\n",
    "    G_pr.remove_edges_from(nx.selfloop_edges(G_pr))\n",
    "    \n",
    "    vk_pr = dict(G_pr.degree())\n",
    "    CLC_pr = nx.closeness_centrality(G_pr)\n",
    "    KC_pr = dict(nx.core_number(G_pr))\n",
    "    B_pr = dict(nx.betweenness_centrality(G_pr))\n",
    "    PR_pr = dict(nx.pagerank(G_pr, alpha=0.85))\n",
    "    EC_pr = dict(nx.eigenvector_centrality(G_pr, max_iter = 1000))\n",
    "                  \n",
    "    year.append(curr_year)\n",
    "    \n",
    "    average_degree_pr.append(momment_of_degree_distribution(G_pr,1))                  #First Moment\n",
    "    second_moment_pr.append(momment_of_degree_distribution(G_pr,2))                   #Second Moment\n",
    "    variance_pr.append(momment_of_degree_distribution(G_pr,2) - momment_of_degree_distribution(G_pr,1)**2)     #Variance\n",
    "    shannon_entropy_pr.append(shannon_entropy(G_pr))                                  #Shanon Entropy\n",
    "    transitivity_pr.append(nx.transitivity(G_pr))                                     #Transitivity \n",
    "    average_cluster_pr.append(nx.average_clustering(G_pr))\n",
    "    #if nx.is_connected(G_pr) == True:\n",
    "    average_shortest_path_length_pr.append(nx.average_shortest_path_length(G_pr))     #Average Shortest Path\n",
    "    diameter_pr.append(nx.diameter(G_pr))                                             #Diameter   \n",
    "    global_efficiency_pr.append(nx.global_efficiency(G_pr))                           #Efficiency\n",
    "    local_efficiency_pr.append(nx.local_efficiency(G_pr))                             #Average Local Efficiency  \n",
    "    average_closeness_pr.append(np.mean(list(CLC_pr.values())))                       #Average closeness centrality\n",
    "    average_betweennes_pr.append(np.mean(list(B_pr.values())))                        #Average betweenness centrality\n",
    "    average_eigenvector_pr.append(np.mean(list(EC_pr.values())))                      #Average eigenvector centrality\n",
    "    average_pagerank_pr.append(np.mean(list(PR_pr.values())))                         #Average PageRank Centrality\n",
    "    assortativity_pr.append(nx.degree_assortativity_coefficient(G_pr))                #Assortativity\n",
    "    \n",
    "    \n",
    "    air_matrix, air_weighted_matrix = weighted_matrix(air[z*365:z*365 + 365,:,:], spei_data)\n",
    "    \n",
    "    G_air = graph_builder(air_weighted_matrix)\n",
    "    G_air = G_air.to_undirected()\n",
    "    G_air.remove_edges_from(nx.selfloop_edges(G_air))\n",
    "    \n",
    "    vk_air = dict(G_air.degree())\n",
    "    CLC_air = nx.closeness_centrality(G_air)\n",
    "    KC_air = dict(nx.core_number(G_air))\n",
    "    B_air = dict(nx.betweenness_centrality(G_air))\n",
    "    PR_air = dict(nx.pagerank(G_air, alpha=0.85))\n",
    "    EC_air = dict(nx.eigenvector_centrality(G_air, max_iter = 1000))\n",
    "                  \n",
    "    average_degree_air.append(momment_of_degree_distribution(G_air,1))                  #First Moment\n",
    "    second_moment_air.append(momment_of_degree_distribution(G_air,2))                   #Second Moment\n",
    "    variance_air.append(momment_of_degree_distribution(G_air,2) - momment_of_degree_distribution(G_air,1)**2)     #Variance\n",
    "    shannon_entropy_air.append(shannon_entropy(G_air))                                  #Shanon Entropy\n",
    "    transitivity_air.append(nx.transitivity(G_air))                                     #Transitivity \n",
    "    average_cluster_air.append(nx.average_clustering(G_air))\n",
    "    #if nx.is_connected(G_air) == True:\n",
    "    average_shortest_path_length_air.append(nx.average_shortest_path_length(G_air))     #Average Shortest Path\n",
    "    diameter_air.append(nx.diameter(G_air))                                             #Diameter   \n",
    "    global_efficiency_air.append(nx.global_efficiency(G_air))                           #Efficiency\n",
    "    local_efficiency_air.append(nx.local_efficiency(G_air))                             #Average Local Efficiency fficiency  \n",
    "    average_closeness_air.append(np.mean(list(CLC_air.values())))          #Average closeness centrality\n",
    "    average_betweennes_air.append(np.mean(list(B_air.values())))           #Average betweenness centrality\n",
    "    average_eigenvector_air.append(np.mean(list(EC_air.values())))         #Average eigenvector centrality\n",
    "    average_pagerank_air.append(np.mean(list(PR_air.values())))\n",
    "    assortativity_air.append(nx.degree_assortativity_coefficient(G_air))                    #Assortativity   \n",
    "    \n",
    "    rows,cols = np.where(~np.isnan(pr_matrix))\n",
    "    nodes = np.unique(rows)\n",
    "    for i in range(np.unique(rows).shape[0]):\n",
    "        lat_index = math.floor(nodes[i]/lon_number)\n",
    "        lon_index = nodes[i] % lon_number\n",
    "        if (i in vk_pr.keys()) and (np.count_nonzero(np.isnan(spei[:,lat_index,lon_index])) == 0):\n",
    "            x = []\n",
    "            x.append(curr_year)\n",
    "            x.append(lat_index)\n",
    "            x.append(lon_index)\n",
    "            x.append(vk_air[i])\n",
    "            x.append(CLC_air[i])\n",
    "            x.append(KC_air[i])\n",
    "            x.append(B_air[i])\n",
    "            x.append(PR_air[i])\n",
    "            x.append(EC_air[i])\n",
    "            x.append(nx.clustering(G_air, i))\n",
    "            x.append(vk_pr[i])\n",
    "            x.append(CLC_pr[i])\n",
    "            x.append(KC_pr[i])\n",
    "            x.append(B_pr[i])\n",
    "            x.append(PR_pr[i])\n",
    "            x.append(EC_pr[i])\n",
    "            x.append(nx.clustering(G_pr, i))\n",
    "            for k in range(13):\n",
    "                x.append(spei[k,lat_index,lon_index])\n",
    "            X.append(x)\n",
    "                \n",
    "    curr_year += 1\n",
    "\n",
    "df_X = pd.DataFrame(X,columns=['year','lat','lon','vk_air','CLC_air','KC_air','B_air','PR_air','EC_air','clustering_air',\n",
    "                                'vk_pr','CLC_pr','KC_pr','B_pr','PR_pr','EC_pr','clustering_pr','SPEI'])\n",
    "    \n",
    "df_pr = pd.DataFrame(list(zip(year, average_degree_pr, second_moment_pr, variance_pr, shannon_entropy_pr, transitivity_pr,\n",
    "                           average_cluster_pr, average_closeness_pr, average_betweennes_pr, average_eigenvector_pr,\n",
    "                           average_pagerank_pr, assortativity_pr,average_shortest_path_length_pr,diameter_pr,global_efficiency_pr,local_efficiency_pr)),\n",
    "                 columns=['year', 'average_degree_pr', 'second_moment_pr', 'variance_pr', 'shannon_entropy_pr',\n",
    "                          'transitivity_pr', 'average_cluster_pr', 'average_closeness_pr', 'average_betweennes_pr',\n",
    "                          'average_eigenvector_pr', 'average_pagerank_pr','assortativity_pr','average_shortest_path_length_pr','diameter_pr,global_efficiency_pr','local_efficiency_pr'])\n",
    "df_air = pd.DataFrame(list(zip(year, average_degree_air, second_moment_air, variance_air, shannon_entropy_air, transitivity_air,\n",
    "                           average_cluster_air, average_closeness_air, average_betweennes_air, average_eigenvector_air,\n",
    "                           average_pagerank_air, assortativity_air,average_shortest_path_length_air,diameter_air,global_efficiency_air,local_efficiency_air)),\n",
    "                 columns=['year', 'average_degree_air', 'second_moment_air', 'variance_air', 'shannon_entropy_air',\n",
    "                          'transitivity_air', 'average_cluster_air','average_closeness_air', 'average_betweennes_air',\n",
    "                          'average_eigenvector_air', 'average_pagerank_air','assortativity_air','average_shortest_path_length_air','diameter_air','global_efficiency_air','local_efficiency_air'])\n",
    "    \n",
    "df_pr.to_csv('pr.csv')    \n",
    "df_air.to_csv('air.csv')\n",
    "df_X.to_csv('Xdata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
